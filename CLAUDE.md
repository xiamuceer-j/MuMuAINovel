# Development Guidelines

## Overview

This document defines project-specific coding standards and development principles.
### CLI Tool Context Protocols
For all CLI tool usage, command syntax, and integration guidelines:
- **MCP Tool Strategy**: @~/.claude/workflows/mcp-tool-strategy.md
- **Intelligent Context Strategy**: @~/.claude/workflows/intelligent-tools-strategy.md
- **Context Search Commands**: @~/.claude/workflows/context-search-strategy.md

**Context Requirements**:
- Identify 3+ existing similar patterns before implementation
- Map dependencies and integration points
- Understand testing framework and coding conventions


## Philosophy

### Core Beliefs

- **Pursue good taste** - Eliminate edge cases to make code logic natural and elegant
- **Embrace extreme simplicity** - Complexity is the root of all evil
- **Be pragmatic** - Code must solve real-world problems, not hypothetical ones
- **Data structures first** - Bad programmers worry about code; good programmers worry about data structures
- **Never break backward compatibility** - Existing functionality is sacred and inviolable
- **Incremental progress over big bangs** - Small changes that compile and pass tests
- **Learning from existing code** - Study and plan before implementing
- **Clear intent over clever code** - Be boring and obvious
- **Follow existing code style** - Match import patterns, naming conventions, and formatting of existing codebase
- **No unsolicited reports** - Task summaries can be performed internally, but NEVER generate additional reports, documentation files, or summary files without explicit user permission

### Simplicity Means

- Single responsibility per function/class
- Avoid premature abstractions
- No clever tricks - choose the boring solution
- If you need to explain it, it's too complex

## Project Integration

### Learning the Codebase

- Find 3 similar features/components
- Identify common patterns and conventions
- Use same libraries/utilities when possible
- Follow existing test patterns

### Tooling

- Use project's existing build system
- Use project's test framework  
- Use project's formatter/linter settings
- Don't introduce new tools without strong justification

## Important Reminders

**NEVER**:
- Make assumptions - verify with existing code
- Generate reports, summaries, or documentation files without explicit user request

**ALWAYS**:
- Plan complex tasks thoroughly before implementation
- Generate task decomposition for multi-module work (>3 modules or >5 subtasks)
- Track progress using TODO checklists for complex tasks
- Validate planning documents before starting development
- Commit working code incrementally
- Update plan documentation and progress tracking as you go
- Learn from existing implementations
- Stop after 3 failed attempts and reassess

## Platform-Specific Guidelines

### Windows Path Format Guidelines
- always use complete absolute Windows paths with drive letters and backslashes for ALL file operations
- **MCP Tools**: Use double backslash `D:\\path\\file.txt` (MCP doesn't support POSIX `/d/path`)
- **Bash Commands**: Use forward slash `D:/path/file.txt` or POSIX `/d/path/file.txt`
- **Relative Paths**: No conversion needed `./src`, `../config`
- **Quick Ref**: `C:\Users` â†’ MCP: `C:\\Users` | Bash: `/c/Users` or `C:/Users`

#### **Content Uniqueness Rules**

- **Each layer owns its abstraction level** - no content sharing between layers
- **Reference, don't duplicate** - point to other layers, never copy content
- **Maintain perspective** - each layer sees the system at its appropriate scale
- **Avoid implementation creep** - higher layers stay architectural

---

## Prompt Management System Integration

### System Overview
The project includes a Prompt Management System running on `localhost:3501` (Docker container: `prompt-manage-prompt-manager-1`).

### Database Architecture
- **Primary Database**: `/app/data/data.sqlite3` (NOT `/app/prompts.db`)
- **Table Structure**: `prompts`, `versions`, `ai_configs`, `optimization_tasks`
- **Current Prompts**:
  - ID 1: "æç¤ºè¯ä¼˜åŒ–"
  - ID 5: "æµ‹è¯•"
  - **Note**: ID 4 (test prompt) does not exist in database

### AI Configuration Status
- **Active Config**: ID 27 - "é¡¹ç›®æ ‡å‡†ä¿®å¤æµ‹è¯•é…ç½®"
- **Provider**: OpenAI-compatible API
- **Model**: gemini-2.5-flash
- **API URL**: `https://newapi.eve.ink/v1`
- **Status**: âœ… Active and configured

### Critical Issues Identified

#### ğŸš¨ **Issue 1: AI API Endpoint Configuration Error**
**Problem**: `Invalid URL (POST /v1)` - 404 Error
**Root Cause**: API URL configuration incomplete or incorrect
**Impact**: All optimization tasks fail with OpenAI service error
**Current Status**: Tasks created successfully but execution fails

```
ERROR: OpenAI æœåŠ¡é”™è¯¯: API è¯·æ±‚å¤±è´¥: 404 - {
  "error": {
    "message": "Invalid URL (POST /v1)",
    "type": "invalid_request_error",
    "param": "",
    "code": ""
  }
}
```

#### ğŸš¨ **Issue 2: Database Connection Instability**
**Problem**: `Cannot operate on a closed database`
**Root Cause**: Database connection management issues
**Impact**: Migration failures and occasional query failures
**Current Status**: Basic functionality works but needs connection pooling

#### ğŸš¨ **Issue 3: Frontend JavaScript Errors**
**Problem**: `_GeneratorContextManager` object attribute error
**Root Cause**: Context manager usage in API endpoints
**Impact**: Some API endpoints return 500 errors
**Affected Endpoints**: `/api/tags` and potentially others

### Functional Testing Results

#### âœ… **Working Components**
- **Basic CRUD Operations**: Create, read, update prompts
- **Web Interface**: Full UI functionality at `http://localhost:3501`
- **API Endpoints**: Most REST endpoints functional
- **Task Creation**: Optimization tasks can be created successfully
- **AI Config Management**: AI configurations load and can be selected

#### âŒ **Non-Working Components**
- **AI Optimization Execution**: Fails due to API URL configuration
- **Database Tags API**: 500 errors due to context manager issues
- **Task Completion**: No successful optimizations completed yet

### API Endpoint Reference

#### Optimization API
```bash
# Create optimization task
POST /api/prompts/<prompt_id>/optimize
Content-Type: application/json
{
  "ai_config_id": 27,
  "optimization_prompt": "è¯·ä¼˜åŒ–è¿™ä¸ªæç¤ºè¯ï¼Œä½¿å…¶æ›´åŠ æœ‰æ•ˆå’Œæ¸…æ™°ã€‚"
}

# Check task status
GET /api/optimization-tasks/<task_id>
```

#### System Status API
```bash
# Get AI configurations
GET /api/ai-configs

# Get prompts list
GET /api/prompts
```

### Resolution Strategies

#### **Priority 1: Fix AI API Configuration**
1. Verify API endpoint URL correctness
2. Test API connectivity with curl/Postman
3. Update configuration in database if needed
4. Validate authentication tokens/keys

#### **Priority 2: Fix Database Connection Issues**
1. Implement proper connection pooling
2. Fix context manager usage in endpoints
3. Add connection error handling and retry logic

#### **Priority 3: Frontend Error Handling**
1. Fix JavaScript context manager implementation
2. Add better error messages for users
3. Improve loading states and progress indicators

### Development Workflow

#### **Testing Optimization Functionality**
1. Use existing prompt ID 1 for testing
2. Monitor task status via `/api/optimization-tasks/<task_id>`
3. Check Docker logs: `docker logs --tail 50 prompt-manage-prompt-manager-1`
4. Verify database state: `docker exec -it prompt-manage-prompt-manager-1 sqlite3 /app/data/data.sqlite3`

#### **Debugging Commands**
```bash
# Check container status
docker ps | grep prompt-manage

# View real-time logs
docker logs -f prompt-manage-prompt-manager-1

# Database inspection
docker exec prompt-manage-prompt-manager-1 sqlite3 /app/data/data.sqlite3 ".tables"

# Test API endpoints
curl -X GET "http://localhost:3501/api/ai-configs"
```

### Resolution Strategies - Updated with MCP Analysis

#### **Priority 1: Fix AI API Configuration** âœ… **RESOLVED**
**Issue**: `Invalid URL (POST /v1)` - 404 Error
**Root Cause**: API endpoint configuration incorrect and missing authentication
**Status**: âœ… **RESOLVED** - API configuration working correctly
**Current config in database**: `api_url = "https://newapi.eve.ink/v1"` (ID: 27)

**Solutions**:
```bash
# Option 1: Fix API URL (Base URL only)
docker exec prompt-manage-prompt-manager-1 sqlite3 /app/data/data.sqlite3 "UPDATE ai_configs SET api_url = 'https://newapi.eve.ink/v1' WHERE id = 27;"

# Option 2: Add API authentication key
# Access: http://localhost:3501/ai-configs or update database directly

# Option 3: Restart service after config changes
docker restart prompt-manage-prompt-manager-1
```

**Verification Commands**:
```bash
# Test API connectivity
curl -X GET "https://newapi.eve.ink/v1/models" -H "Authorization: Bearer YOUR_API_KEY"

# Test optimization after fix
curl -X POST "http://localhost:3501/api/prompts/1/optimize" \
  -H "Content-Type: application/json" \
  -d '{"ai_config_id": 27, "optimization_prompt": "è¯·ä¼˜åŒ–è¿™ä¸ªæç¤ºè¯"}'
```

#### **Priority 2: Core Python Errors** âœ… **RESOLVED (2025-11-08)**
**Status**: âœ… **RESOLVED** - Three critical Python errors fixed successfully

**Fixed Issues**:
1. **sqlite3.Row AttributeError** âœ… **RESOLVED**
   - **Problem**: `'sqlite3.Row' object has no attribute 'get'`
   - **Location**: Multiple functions in `/app/app.py`
   - **Solution**: Convert sqlite3.Row objects to dictionaries using `dict(task)` before accessing with `.get()`
   - **Functions Fixed**:
     - `AIService.__init__()` - Line ~1379
     - `create_ai_service()` - Line ~1522
     - `run_optimization_async()` - Line ~1550

2. **Database Context Manager Error** âœ… **RESOLVED**
   - **Problem**: `'_GeneratorContextManager' object has no attribute 'execute'`
   - **Location**: `api_tags()` function - Line ~1278
   - **Solution**: Changed from `conn = get_db()` to `with get_db() as conn:`

3. **Version Number Type Conversion Error** âœ… **RESOLVED**
   - **Problem**: `could not convert string to float: '1.0.0'`
   - **Location**: `run_optimization_async()` - Line ~1590
   - **Solution**: Added proper version string handling logic to safely parse version numbers

**Current Status**: âœ… Optimization workflow now executes successfully through task creation and AI processing phases. Minor database schema issue remains.

#### **Priority 3: Database Schema Issues** ğŸ”„ **IN PROGRESS**
**Current Issue**: `table version_relations has no column named optimization_task_id`
**Status**: Tasks complete AI processing but fail during result insertion due to schema mismatch
**Next Steps**: Need to add missing column or fix database migration logic

#### **Priority 4: Frontend Error Handling**
- Fix JavaScript context manager implementation
- Add better error messages for users
- Improve loading states and progress indicators

---

## ğŸ³ Dockeræ„å»ºç¼“å­˜ç­–ç•¥æŒ‡å—

### ğŸ“‹ **ä½•æ—¶ä½¿ç”¨ç¼“å­˜ï¼ˆæ¨èï¼‰**

#### âœ… **ä½¿ç”¨ç¼“å­˜çš„æƒ…å†µ**
```bash
# æ¨èå‘½ä»¤ï¼šåˆ©ç”¨Dockerå±‚ç¼“å­˜ï¼Œæ„å»ºé€Ÿåº¦å¿«
docker compose build mumuainovel
```

**é€‚ç”¨åœºæ™¯**ï¼š
- **æ—¥å¸¸å¼€å‘**ï¼šåªæœ‰å°‘é‡ä»£ç å˜æ›´
- **CI/CDæµæ°´çº¿**ï¼šä¾èµ–æ–‡ä»¶æœªå˜åŒ–
- **å¿«é€Ÿæµ‹è¯•**ï¼šéœ€è¦å¿«é€Ÿæ„å»ºéªŒè¯
- **èµ„æºä¼˜åŒ–**ï¼šèŠ‚çœæ„å»ºæ—¶é—´å’Œå¸¦å®½

**æ•ˆæœ**ï¼š
- **é¦–æ¬¡æ„å»º**ï¼š10åˆ†é’Ÿï¼ˆæ— ç¼“å­˜ï¼‰
- **åç»­æ„å»º**ï¼š1-3åˆ†é’Ÿï¼ˆæœ‰ç¼“å­˜ï¼‰
- **ä»…åç«¯å˜æ›´**ï¼š30ç§’ï¼ˆå‰ç«¯ç¼“å­˜å‘½ä¸­ï¼‰
- **ä»…å‰ç«¯å˜æ›´**ï¼š1åˆ†é’Ÿï¼ˆåç«¯ç¼“å­˜å‘½ä¸­ï¼‰

#### ğŸ¯ **ç¼“å­˜å‘½ä¸­çš„å…³é”®**
1. **ä¾èµ–æ–‡ä»¶ä¸å˜**ï¼š`package.json`ã€`requirements.txt`
2. **ç³»ç»Ÿä¾èµ–ä¸å˜**ï¼šDockerfileåŸºç¡€å±‚
3. **æ¨¡å‹æ–‡ä»¶ä¸å˜**ï¼š`backend/embedding/`
4. **é…ç½®æ–‡ä»¶ä¸å˜**ï¼š`.env`ã€`docker-compose.yml`

### âŒ **ä¸ä½¿ç”¨ç¼“å­˜ï¼ˆå¿…è¦æ—¶ï¼‰**

#### ğŸš¨ **å¿…é¡»ä½¿ç”¨--no-cacheçš„æƒ…å†µ**
```bash
# å¿…è¦å‘½ä»¤ï¼šå®Œå…¨é‡æ–°æ„å»ºï¼Œç¡®ä¿ä¸€è‡´æ€§
docker compose build mumuainovel --no-cache
```

**é€‚ç”¨åœºæ™¯**ï¼š
- **é¦–æ¬¡æ„å»º**ï¼šå…¨æ–°ç¯å¢ƒï¼Œæ— ç¼“å­˜å¯ç”¨
- **ä¾èµ–å˜æ›´**ï¼š`package.json`ã€`requirements.txt`æ›´æ–°
- **åŸºç¡€é•œåƒæ›´æ–°**ï¼šç³»ç»Ÿä¾èµ–æˆ–Pythonç‰ˆæœ¬å‡çº§
- **ç¼“å­˜é—®é¢˜**ï¼šæ„å»ºå‡ºç°è«åå…¶å¦™çš„é”™è¯¯
- **ç”Ÿäº§å‘å¸ƒ**ï¼šç¡®ä¿å®Œå…¨å¹²å‡€çš„æ„å»º
- **å¤§ç‰ˆæœ¬å‡çº§**ï¼šNode.jsã€Pythonã€ä¾èµ–åº“å¤§ç‰ˆæœ¬å‡çº§

#### ğŸ” **éœ€è¦--no-cacheçš„ç—‡çŠ¶**
- æ„å»ºæˆåŠŸä½†è¿è¡Œæ—¶å‡ºç°è«åé”™è¯¯
- ä¾èµ–ç‰ˆæœ¬å†²çª
- æŸäº›åŒ…å®‰è£…å¤±è´¥
- è¿è¡Œæ—¶æ‰¾ä¸åˆ°æ¨¡å—
- ç¼“å­˜å±‚æŸåå¯¼è‡´çš„é—®é¢˜

### ğŸ”„ **æ™ºèƒ½ç¼“å­˜ç­–ç•¥**

#### **æœ€ä½³å®è·µå·¥ä½œæµ**

```bash
# 1. æ—¥å¸¸å¼€å‘ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
docker compose build mumuainovel

# 2. å¦‚æœæ„å»ºå¤±è´¥ï¼Œå°è¯•æ¸…ç†ç›¸å…³å±‚
docker compose build --no-cache mumuainovel

# 3. ä¾èµ–æ›´æ–°åï¼Œæ¸…ç†ç¼“å­˜
# æ›´æ–° package.json æˆ– requirements.txt å
docker compose build --no-cache mumuainovel

# 4. å®šæœŸæ¸…ç†ï¼ˆæ¯å‘¨/æ¯æœˆï¼‰
# é¿å…ç¼“å­˜å±‚è¿‡å¤šå½±å“æ„å»º
docker system prune -f
```

#### **ç¼“å­˜é—®é¢˜è¯Šæ–­**

```bash
# æ£€æŸ¥Dockerå­˜å‚¨ä½¿ç”¨æƒ…å†µ
docker system df

# æ¸…ç†æœªä½¿ç”¨çš„é•œåƒå’Œç¼“å­˜
docker system prune -a -f

# å¼ºåˆ¶æ¸…ç†æ‰€æœ‰ç¼“å­˜
docker builder prune -a -f
```

### ğŸ’¡ **ç¼“å­˜ä¼˜åŒ–æŠ€å·§**

#### **ä¼˜åŒ–Dockerfileå±‚çº§**
- **å°‘å˜å±‚åœ¨å‰**ï¼šç³»ç»Ÿä¾èµ–ã€Pythonç¯å¢ƒ
- **å¤šå˜å±‚åœ¨å**ï¼šåº”ç”¨ä»£ç ã€é…ç½®æ–‡ä»¶
- **åˆç†åˆ†ç»„**ï¼šç›¸å…³æ“ä½œåˆå¹¶åˆ°åŒä¸€å±‚

#### **ç›‘æ§ç¼“å­˜æ•ˆæœ**
```bash
# æŸ¥çœ‹æ„å»ºæ—¶é—´
time docker compose build mumuainovel

# æŸ¥çœ‹ç¼“å­˜å‘½ä¸­æƒ…å†µ
docker compose build --progress=plain mumuainovel
```

### ğŸ¯ **å†³ç­–æŒ‡å—**

| åœºæ™¯ | æ¨èå‘½ä»¤ | åŸå›  |
|------|----------|------|
| **æ—¥å¸¸å¼€å‘** | `docker compose build` | åˆ©ç”¨ç¼“å­˜ï¼Œå¿«é€Ÿè¿­ä»£ |
| **ä¾èµ–æ›´æ–°** | `docker compose build --no-cache` | ç¡®ä¿æ–°ä¾èµ–æ­£ç¡®å®‰è£… |
| **é¦–æ¬¡éƒ¨ç½²** | `docker compose build --no-cache` | æ— ç¼“å­˜å¯ç”¨ï¼Œç¡®ä¿å®Œæ•´ |
| **æ„å»ºé”™è¯¯** | `docker compose build --no-cache` | æ’é™¤ç¼“å­˜é—®é¢˜ |
| **ç”Ÿäº§å‘å¸ƒ** | `docker compose build --no-cache` | ç¡®ä¿ç”Ÿäº§ç¯å¢ƒä¸€è‡´æ€§ |
| **å¿«é€Ÿæµ‹è¯•** | `docker compose build` | é€Ÿåº¦ä¼˜å…ˆï¼ŒèŠ‚çœæ—¶é—´ |

### âš ï¸ **æ³¨æ„äº‹é¡¹**

1. **ç¼“å­˜ä¾èµ–æ–‡ä»¶å˜æ›´æ—¶å¿…é¡»ç”¨--no-cache**
2. **ç”Ÿäº§ç¯å¢ƒå‘å¸ƒå»ºè®®ç”¨--no-cacheç¡®ä¿ä¸€è‡´æ€§**
3. **å®šæœŸæ¸…ç†Dockerç¼“å­˜é¿å…å­˜å‚¨ç©ºé—´é—®é¢˜**
4. **æ„å»ºå¤±è´¥æ—¶ä¼˜å…ˆå°è¯•--no-cache**
5. **å›¢é˜Ÿå¼€å‘æ—¶ä¿æŒDockerfileç‰ˆæœ¬ä¸€è‡´**

**è®°ä½ï¼šç¼“å­˜æ˜¯åŒåˆƒå‰‘ï¼Œç”¨å¾—å¥½å¤§å¹…æå‡æ•ˆç‡ï¼Œç”¨é”™äº†å¸¦æ¥å›°æ‰°ï¼**

### MCP-Enhanced Debugging Workflow

#### **Using MCP Tools for Advanced Analysis**
The investigation leveraged multiple MCP tools for comprehensive analysis:

1. **Sequential Thinking Tools**: Systematic problem breakdown and solution path identification
2. **Context7 Documentation**: OpenAI API official documentation retrieval for correct endpoint configuration
3. **Tavily Web Crawling**: Real-time API documentation and community solution discovery

#### **MCP Analysis Methodology**
```bash
# Tool usage pattern for complex debugging
mcp__mcphub__search_tools  # Discover relevant analysis tools
mcp__sequentialthinking-tools  # Systematic problem analysis
context7-get-library-docs  # Official documentation retrieval
tavily-mcp-tavily-crawl  # Web resource discovery
```

### Updated Resolution Strategies

#### **Immediate Action Items**
1. **API Configuration Fix**: Update AI config with correct base URL and authentication
2. **Client Code Review**: Ensure OpenAI client uses proper base_url configuration
3. **Authentication Setup**: Obtain and configure valid API keys for newapi.eve.ink

#### **Alternative Solutions**
If current API provider cannot be fixed:
- **Primary Alternative**: OpenAI official API (`https://api.openai.com/v1`)
- **Local Options**: Ollama, LocalAI, or vLLM for on-premise deployment
- **Other Providers**: Various OpenAI-compatible API services

#### **Verification Protocol**
After fixes applied:
1. Test API connectivity with authentication
2. Create optimization task via API
3. Monitor task completion and results
4. Validate frontend integration

### Future Improvements - Enhanced with MCP Insights
- Add API endpoint validation and health checks (automated MCP-based testing)
- Implement proper error logging and monitoring (MCP tool integration)
- Add frontend validation for AI configuration (MCP-driven form validation)
- Create automated testing for optimization workflow (MCP sequential thinking)
- Add support for multiple AI providers beyond OpenAI-compatible APIs
- **NEW**: Implement MCP-based diagnostic tools for real-time system health monitoring
- **NEW**: Add automated API endpoint verification using MCP web crawling capabilities

---

## ğŸ“‹ Gité¡¹ç›®æ›´æ–°æ ‡å‡†æµç¨‹

### ğŸš¨ **è¡€æ³ªæ•™è®­ï¼š2025-11-14 MuMuAINovelæ›´æ–°ç¾éš¾**

#### **ç¾éš¾å›é¡¾**
- **æ ¹æœ¬é”™è¯¯**ï¼šè¯¯åˆ¤ç½‘ç»œè¿æ¥é—®é¢˜ä¸ºåˆ†æ”¯å†²çªï¼Œå¯¼è‡´åˆ é™¤420MBå®è´µæ¨¡å‹æ–‡ä»¶
- **è¿é”é”™è¯¯**ï¼šåœ¨é”™è¯¯æ–¹å‘ä¸Šä¸æ–­å°è¯•ï¼Œæµªè´¹å¤§é‡æ—¶é—´
- **è§£å†³ä»£ä»·**ï¼šä¾èµ–å…¶ä»–é¡¹ç›®çš„æ¨¡å‹æ–‡ä»¶æ‰æ¢å¤

#### **æ ¸å¿ƒæ•™è®­**
1. **å…ˆè¯Šæ–­åè§£å†³**ï¼šé”™è¯¯è¯Šæ–­å¯¼è‡´é”™è¯¯è§£å†³æ–¹æ¡ˆ
2. **ç½‘ç»œä¼˜å…ˆ**ï¼šGité—®é¢˜å…ˆæ£€æŸ¥ç½‘ç»œè¿æ¥
3. **å¤§æ–‡ä»¶å³èµ„äº§**ï¼šä»»ä½•å‡ ç™¾MBçš„æ–‡ä»¶éƒ½åº”è§†ä¸ºçè´µèµ„æº
4. **æ°¸è¿œå¤‡ä»½**ï¼šæ‰§è¡Œç ´åæ€§å‘½ä»¤å‰å¿…é¡»å¤‡ä»½

### ğŸ”„ **æ ‡å‡†æ›´æ–°æµç¨‹ï¼ˆå››é˜¶æ®µï¼‰**

#### **ç¬¬ä¸€é˜¶æ®µï¼šç¯å¢ƒè¯Šæ–­ï¼ˆå¿…é¡»ï¼ï¼‰**

```bash
# 1. ç½‘ç»œè¿æ¥è¯Šæ–­
curl -I https://github.com > /dev/null 2>&1 && echo "âœ… GitHubè¿æ¥æ­£å¸¸" || echo "âŒ éœ€è¦ä»£ç†"

# 2. Gitä»“åº“çŠ¶æ€æ£€æŸ¥
git status
git remote -v
git branch -a

# 3. å¤§æ–‡ä»¶èµ„äº§æ£€æŸ¥ï¼ˆå…³é”®ï¼ï¼‰
du -sh */embedding/ 2>/dev/null || echo "æ— embeddingç›®å½•"
find . -name "*.safetensors" -exec ls -lh {} \; 2>/dev/null || echo "æ— æ¨¡å‹æ–‡ä»¶"

# 4. å®¹å™¨è¿è¡ŒçŠ¶æ€
docker compose ps
```

**å¦‚æœå‘ç°é—®é¢˜**ï¼š
- âŒ ç½‘ç»œé—®é¢˜ â†’ é…ç½®ä»£ç†åå†ç»§ç»­
- âŒ å¤§æ–‡ä»¶ç¼ºå¤± â†’ å…ˆå¤‡ä»½æˆ–æ¢å¤ï¼Œå†è€ƒè™‘æ›´æ–°

#### **ç¬¬äºŒé˜¶æ®µï¼šå®‰å…¨å‡†å¤‡**

```bash
# 1. å¤‡ä»½é‡è¦èµ„äº§
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
if [ -d "backend/embedding" ] && [ "$(du -s backend/embedding | cut -f1)" -gt 100000 ]; then
    echo "ğŸ’¾ å¤‡ä»½embeddingæ¨¡å‹..."
    cp -r backend/embedding backend/embedding_backup_$TIMESTAMP
    echo "âœ… æ¨¡å‹æ–‡ä»¶å·²å¤‡ä»½åˆ° backend/embedding_backup_$TIMESTAMP"
fi

# 2. æŸ¥çœ‹å³å°†æ‹‰å–çš„å˜æ›´
git fetch origin main 2>/dev/null || echo "âš ï¸  ç½‘ç»œé—®é¢˜ï¼Œæ— æ³•è·å–è¿œç¨‹ä¿¡æ¯"
if [ $? -eq 0 ]; then
    echo "ğŸ“‹ å³å°†æ‹‰å–çš„å˜æ›´ï¼š"
    git log --oneline HEAD..origin/main
    git diff --name-status HEAD..origin/main
fi
```

#### **ç¬¬ä¸‰é˜¶æ®µï¼šæ™ºèƒ½æ›´æ–°**

```bash
# 1. ä¼˜å…ˆä½¿ç”¨å¢é‡æ›´æ–°ï¼ˆå¸¦ä»£ç†ï¼‰
if command -v HTTPS_PROXY >/dev/null; then
    echo "ğŸ”„ å°è¯•å¢é‡æ›´æ–°..."
    HTTPS_PROXY=http://127.0.0.1:7897 git pull origin main
    if [ $? -eq 0 ]; then
        echo "âœ… å¢é‡æ›´æ–°æˆåŠŸ"
        return 0
    fi
fi

# 2. å¦‚æœLFSé—®é¢˜ï¼Œè·³è¿‡å¤§æ–‡ä»¶
echo "âš ï¸  å°è¯•è·³è¿‡LFSå¤§æ–‡ä»¶..."
GIT_LFS_SKIP_SMUDGE=1 HTTPS_PROXY=http://127.0.0.1:7897 git pull origin main
if [ $? -eq 0 ]; then
    echo "âœ… è·³è¿‡å¤§æ–‡ä»¶æ›´æ–°æˆåŠŸï¼Œéœ€æ‰‹åŠ¨å¤„ç†æ¨¡å‹"
    return 0
fi

# 3. æœ€åæ‰‹æ®µï¼šæ‰‹åŠ¨å¤„ç†å¤§æ–‡ä»¶
echo "ğŸ”§ æ‰‹åŠ¨å¤„ç†å¤§æ–‡ä»¶å†²çª..."
# æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–é¡¹ç›®æœ‰å®Œæ•´æ–‡ä»¶
for PROJECT_DIR in /vol1/1000/docker/*/; do
    if [ -d "$PROJECT_DIR/backend/embedding" ] && [ "$(ls -la $PROJECT_DIR/backend/embedding/*.safetensors 2>/dev/null | wc -l)" -gt 0 ]; then
        echo "ğŸ“ å‘ç°å®Œæ•´æ¨¡å‹æ–‡ä»¶åœ¨ï¼š$PROJECT_DIR"
        read -p "æ˜¯å¦å¤åˆ¶åˆ°æ­¤é¡¹ç›®ï¼Ÿ(y/n): " -n 1 -r
        echo
        if [[ $REPLY =~ ^[Yy]$ ]]; then
            cp -r $PROJECT_DIR/backend/embedding/* backend/embedding/
            echo "âœ… æ¨¡å‹æ–‡ä»¶å·²å¤åˆ¶"
            break
        fi
    fi
done
```

#### **ç¬¬å››é˜¶æ®µï¼šæ„å»ºéªŒè¯**

```bash
# 1. å¢é‡æ„å»ºï¼ˆä¼˜å…ˆï¼‰
echo "ğŸ”¨ å°è¯•å¢é‡æ„å»º..."
docker compose build mumuainovel
if [ $? -eq 0 ]; then
    echo "âœ… å¢é‡æ„å»ºæˆåŠŸ"
else
    echo "âš ï¸  å¢é‡æ„å»ºå¤±è´¥ï¼Œå°è¯•å®Œæ•´é‡å»º..."
    docker compose build mumuainovel --no-cache
fi

# 2. æœåŠ¡é‡å¯
echo "ğŸ”„ é‡å¯æœåŠ¡..."
docker compose restart mumuainovel

# 3. ç­‰å¾…å¯åŠ¨
echo "â³ ç­‰å¾…æœåŠ¡å¯åŠ¨..."
sleep 30

# 4. å¥åº·æ£€æŸ¥
echo "ğŸ¥ æ‰§è¡Œå¥åº·æ£€æŸ¥..."
if curl -f http://localhost:8025/health >/dev/null 2>&1; then
    echo "âœ… æœåŠ¡å¯åŠ¨æˆåŠŸ"
    echo "ğŸŒ è®¿é—®åœ°å€: http://localhost:8025"
else
    echo "âŒ æœåŠ¡å¯åŠ¨å¤±è´¥ï¼Œæ£€æŸ¥æ—¥å¿—ï¼š"
    docker logs mumuainovel --tail 20
fi
```

### ğŸš¨ **ç»å¯¹ç¦æ­¢çš„æ“ä½œï¼ˆçº¢è‰²è­¦æŠ¥ï¼‰**

```bash
# âŒ é™¤é100%ç¡®å®šï¼Œå¦åˆ™ç¦æ­¢æ‰§è¡Œï¼š
git clean -fd                    # å·²å¯¼è‡´ç¾éš¾
git reset --hard HEAD~1         # é™¤éæœ‰å®Œæ•´å¤‡ä»½
rm -rf backend/embedding/       # 420MBæ¨¡å‹æ–‡ä»¶
docker system prune -af         # å¯èƒ½åˆ é™¤é‡è¦é•œåƒ

# âœ… å®‰å…¨çš„æ›¿ä»£æ–¹æ¡ˆï¼š
git stash push -m "backup"       # å®‰å…¨æš‚å­˜
git add . && git commit          # å…ˆæäº¤å†æ“ä½œ
mv backend/embedding backup/    # ç§»åŠ¨è€Œéåˆ é™¤
```

### ğŸ“Š **åº”æ€¥æ¢å¤æ¸…å•**

å¦‚æœæ›´æ–°å¤±è´¥ï¼ŒæŒ‰é¡ºåºæ£€æŸ¥ï¼š

```bash
# 1. æ¢å¤æ¨¡å‹æ–‡ä»¶
if [ -d "backend/embedding_backup_*" ]; then
    LATEST_BACKUP=$(ls -td backend/embedding_backup_* | head -1)
    echo "ğŸ”„ ä»å¤‡ä»½æ¢å¤æ¨¡å‹: $LATEST_BACKUP"
    rm -rf backend/embedding
    cp -r "$LATEST_BACKUP" backend/embedding
fi

# 2. å›æ»šä»£ç 
git reflog --oneline -10  # æŸ¥çœ‹æœ€è¿‘æ“ä½œ
git reset --hard <commit-hash>  # å›æ»šåˆ°å·¥ä½œçŠ¶æ€

# 3. é‡æ–°æ„å»º
docker compose build mumuainovel --no-cache
docker compose up -d mumuainovel
```

### ğŸ¯ **æˆåŠŸæ ‡å‡†**

æ›´æ–°æˆåŠŸå¿…é¡»æ»¡è¶³ï¼š
- âœ… ç½‘ç»œè¿æ¥æ­£å¸¸æˆ–ä»£ç†é…ç½®æœ‰æ•ˆ
- âœ… é‡è¦æ–‡ä»¶ï¼ˆç‰¹åˆ«æ˜¯embeddingæ¨¡å‹ï¼‰å®Œæ•´æ— æŸ
- âœ… ä»£ç æˆåŠŸæ‹‰å–åˆ°æœ€æ–°ç‰ˆæœ¬
- âœ… Dockeræ„å»ºæ— é”™è¯¯
- âœ… å®¹å™¨å¯åŠ¨ä¸”å¥åº·æ£€æŸ¥é€šè¿‡
- âœ… åº”ç”¨åŠŸèƒ½æ­£å¸¸è®¿é—®

### ğŸ“ **æ›´æ–°åè®°å½•**

```bash
echo "$(date): æ›´æ–°å®Œæˆ" >> update.log
echo "Commit: $(git rev-parse --short HEAD)" >> update.log
echo "é•œåƒ: $(docker images mumuainovel -q)" >> update.log
echo "çŠ¶æ€: $(curl -s http://localhost:8025/health | jq .status)" >> update.log
```

**è®°ä½**ï¼šå®å¯å¤šèŠ±10åˆ†é’Ÿæ£€æŸ¥ï¼Œä¹Ÿä¸è¦èŠ±2å°æ—¶æ•‘ç¾ï¼

